{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51050f7b",
   "metadata": {},
   "source": [
    "# Name : Yenumula Pavan Gopal Mourya\n",
    "# Roll no :202051208\n",
    "# Exam : AI_Lab_Exam_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89986b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy generated after Value Iteration:\n",
      "{1: 'STAY', 2: 'STAY', 3: 'STAY', 4: 'STAY', 5: 'STAY', 6: 'STAY', 7: 'STAY', 8: 'QUIT', 9: 'QUIT', 10: None, 11: 'STAY'}\n",
      "Values generated after Value Iteration:\n",
      "{1: 26780.771381760005, 2: 29945.87136000001, 3: 36414.656, 4: 49464.799999999996, 5: 72960.0, 6: 124000.0, 7: 220000.0, 8: 500000.0, 9: 1000000.0, 10: 0.0, 11: 0.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Thresh hold value\n",
    "epsilon=0.0001\n",
    "# Discount Factor\n",
    "Gamma=0.9\n",
    "# no.of question\n",
    "No_of_Questions = 10\n",
    "# States and corresponding rewards and probabilities assosciated\n",
    "Rewards = dict([(1, (0.99, 100)), \n",
    "                     (2, (0.9, 500)), \n",
    "                     (3, (0.8, 1000)), \n",
    "                     (4, (0.7, 5000)), \n",
    "                     (5, (0.6, 10000)),\n",
    "                     (6, (0.5, 50000)), \n",
    "                     (7, (0.4, 100000)), \n",
    "                     (8, (0.3, 500000)), \n",
    "                     (9, (0.2, 1000000)), \n",
    "                     (10, (0.1, 5000000))])\n",
    "\n",
    "Trans = np.array([[0.99, 0.01, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [0, 0.9, 0.1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [0, 0, 0.8, 0.2, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 0.7, 0.3, 0, 0, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0.6, 0.4, 0, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0.5, 0.5, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0.4, 0.6, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0.3, 0.7, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0.8, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.9],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "\n",
    "# CLass of Markov Decision Process\n",
    "class MDP():\n",
    "\n",
    "  # Checking if the state is terminal or not\n",
    "  def Is_Terminal(self, state):\n",
    "    return True if state == No_of_Questions else False\n",
    "  # THere are two possible actions at each state\n",
    "  def Possible_actions(self,state):\n",
    "    return [] if self.Is_Terminal(state) else ['STAY', 'QUIT']\n",
    "  # Just a boolean function\n",
    "  def State_Intialization(self):\n",
    "    return 1\n",
    "  \n",
    "  def Rounds(self):\n",
    "    return [i for i in range(1, No_of_Questions+2)]\n",
    "    \n",
    "  def Reward_S(self, state, action):\n",
    "        return [(state, 1., 0.)] if state > No_of_Questions else [(state+1, Rewards[state][0], Rewards[state][1]), (No_of_Questions+1, 1.-Rewards[state][0], 0.)] if action == 'STAY' else [(No_of_Questions+1, 1.0, Rewards[state][1])] if state <= No_of_Questions else [(state, 1., 0.)]\n",
    "\n",
    "\n",
    "# We will make use of bellman equation\n",
    "# The solution will converge . We can prove it using fixed point theorem\n",
    "# As mapping of Bellman equation is of contracting nature\n",
    "\n",
    "def value_iteration(mdp):\n",
    "    # At the begining fill the value matrix with zeroes\n",
    "    values = dict(zip(mdp.Rounds(), [0.0] * len(mdp.Rounds())))\n",
    "\n",
    "    # We will keep on iterating untill we converge\n",
    "    while True:\n",
    "        # Difference\n",
    "        delta = 0\n",
    "\n",
    "        # Value updation using Bellman's theory\n",
    "        for state in mdp.Rounds():\n",
    "            if mdp.Is_Terminal(state):\n",
    "                continue\n",
    "\n",
    "            # Initially intialize the max value with - Inifinity\n",
    "            max_value = -float('inf')\n",
    "            for action in mdp.Possible_actions(state):\n",
    "                value = sum(map(lambda x: x[1] * (x[2] + Gamma * values[x[0]]), mdp.Reward_S(state, action)))\n",
    "                max_value = max(max_value, value)\n",
    "            # Value updation when absolute difference of  max_value - values[state] is greater than delta\n",
    "            delta = max(delta, abs(max_value - values[state]))\n",
    "            values[state] = max_value\n",
    "\n",
    "        # Checking for convergence\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "\n",
    "    # Optimal Policy Computation\n",
    "    policy = {}\n",
    "    for state in mdp.Rounds():\n",
    "        if mdp.Is_Terminal(state):\n",
    "            policy[state] = None\n",
    "        else:\n",
    "            best_action = max(mdp.Possible_actions(state),\n",
    "                              key=lambda action: sum(probability * (reward + Gamma * values[next_state])\n",
    "                                                     for next_state, probability, reward in mdp.Reward_S(state, action)))\n",
    "            policy[state] = best_action\n",
    "\n",
    "    # Return the computed values and policy.\n",
    "    return values, policy\n",
    "\n",
    "instance = MDP()\n",
    "\n",
    "Values ,Policy = value_iteration(instance)\n",
    "\n",
    "print('Policy generated after Value Iteration:')\n",
    "print(Policy)\n",
    "print('Values generated after Value Iteration:')\n",
    "print(Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5d7ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy generated after Policy Iteration:\n",
      "{1: 'STAY', 2: 'STAY', 3: 'STAY', 4: 'STAY', 5: 'STAY', 6: 'STAY', 7: 'STAY', 8: 'QUIT', 9: 'QUIT', 11: 'STAY'}\n",
      "Values generated after Policy Iteration:\n",
      "{1: 26780.771381760005, 2: 29945.87136000001, 3: 36414.656, 4: 49464.799999999996, 5: 72960.0, 6: 124000.0, 7: 220000.0, 8: 500000.0, 9: 1000000.0, 10: 0.0, 11: 0.0}\n"
     ]
    }
   ],
   "source": [
    "def policyIteration(mdp):\n",
    "    # At the begining fill the value matrix with zeroes\n",
    "    Value =dict(zip(mdp.Rounds(), [0.0] * len(mdp.Rounds())))\n",
    "\n",
    "    Policy = {s: mdp.Possible_actions(s)[0] for s in mdp.Rounds() if not mdp.Is_Terminal(s)}\n",
    "    # Infinite Loop until broken\n",
    "    while True:\n",
    "         # Infinite Loop until broken (due to convergence)\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in mdp.Rounds():\n",
    "                if mdp.Is_Terminal(s):\n",
    "                    continue\n",
    "                Valueal = 0\n",
    "                for NextState, Probability, Reward in mdp.Reward_S(s, Policy[s]):\n",
    "                    Valueal += Probability * (Reward + Gamma * Value[NextState])\n",
    "                # update Valuealue of state\n",
    "                delta = max(delta, abs(Valueal - Value[s]))\n",
    "                Value[s] = Valueal\n",
    "            # When the delta is not signifcant break the infinite loop\n",
    "            # epsilon is the thresh hold value\n",
    "            if delta < epsilon:\n",
    "                break\n",
    "        # Improvement of policy\n",
    "        policy_stable = True\n",
    "        for s in mdp.Rounds():\n",
    "            if mdp.Is_Terminal(s):\n",
    "                continue\n",
    "            old_action = Policy[s]\n",
    "            # Initially intializing with minus infinity\n",
    "            MaxValuealue = -float('inf')\n",
    "            best_action = None\n",
    "            for a in mdp.Possible_actions(s):\n",
    "                Valueal = 0\n",
    "                # For each state in the MDP, compute the value of each possible action under the updated values.\n",
    "                for NextState, Probability, Reward in mdp.Reward_S(s, a):\n",
    "                    Valueal += Probability * (Reward + Gamma * Value[NextState])\n",
    "                if Valueal > MaxValuealue:\n",
    "                    MaxValuealue = Valueal\n",
    "                    best_action = a\n",
    "            # Policy Updation\n",
    "            # Update the policy to choose the action that has the highest computed value.\n",
    "            Policy[s] = best_action\n",
    "            if old_action != best_action:\n",
    "                # If the policy has changed, set policy_stable to False.\n",
    "                policy_stable = False\n",
    "        # If the policy is stable then exit\n",
    "        if policy_stable:\n",
    "            break\n",
    "    # Returning the tuple of corresponding values\n",
    "    return Value, Policy\n",
    "\n",
    "instance = MDP()\n",
    "\n",
    "Values ,Policy = policyIteration(instance)\n",
    "\n",
    "print('Policy generated after Policy Iteration:')\n",
    "print(Policy)\n",
    "print('Values generated after Policy Iteration:')\n",
    "print(Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97a7216d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences :\n",
      "\n",
      "Sequence 1 :\n",
      "[(1, 'STAY', 100, 2), (2, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11)]\n",
      "\n",
      "Sequence 2 :\n",
      "[(1, 'STAY', 100, 2), (2, 'STAY', 500, 3), (3, 'STAY', 1000, 4), (4, 'STAY', 5000, 5), (5, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11)]\n",
      "\n",
      "Sequence 3 :\n",
      "[(1, 'STAY', 100, 2), (2, 'STAY', 500, 3), (3, 'STAY', 1000, 4), (4, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11)]\n",
      "\n",
      "Sequence 4 :\n",
      "[(1, 'STAY', 100, 2), (2, 'STAY', 500, 3), (3, 'STAY', 1000, 4), (4, 'STAY', 5000, 5), (5, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11)]\n",
      "\n",
      "Sequence 5 :\n",
      "[(1, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11)]\n",
      "\n",
      "Sequence 6 :\n",
      "[(1, 'STAY', 100, 2), (2, 'STAY', 500, 3), (3, 'STAY', 1000, 4), (4, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11)]\n",
      "\n",
      "Sequence 7 :\n",
      "[(1, 'STAY', 100, 2), (2, 'STAY', 500, 3), (3, 'STAY', 1000, 4), (4, 'STAY', 5000, 5), (5, 'STAY', 10000, 6), (6, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11)]\n",
      "\n",
      "Sequence 8 :\n",
      "[(1, 'STAY', 100, 2), (2, 'STAY', 500, 3), (3, 'STAY', 1000, 4), (4, 'STAY', 5000, 5), (5, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11)]\n",
      "\n",
      "Sequence 9 :\n",
      "[(1, 'STAY', 100, 2), (2, 'STAY', 500, 3), (3, 'STAY', 1000, 4), (4, 'STAY', 5000, 5), (5, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11)]\n",
      "\n",
      "Sequence 10 :\n",
      "[(1, 'STAY', 100, 2), (2, 'STAY', 500, 3), (3, 'STAY', 1000, 4), (4, 'STAY', 5000, 5), (5, 'STAY', 10000, 6), (6, 'STAY', 50000, 7), (7, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11), (11, 'STAY', 0.0, 11)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max_steps: an integer specifying the maximum number of steps to take in each sequence. Defaults to 50.\n",
    "# pi: a dictionary mapping each state to an action, representing the policy used to generate the sequences.\n",
    "# num_sequences: an integer specifying the number of sequences to generate. Defaults to 1.\n",
    "def generateSARS(instance, policy, num_sequences=1, max_steps=50):\n",
    "    # Generate SARS tuples (state, action, reward, next_state) for a given instance and policy. \n",
    "    sequences = []\n",
    "    #It does this by iterating over the specified number of sequences and generating a sequence for each one. \n",
    "    for i in range(num_sequences):\n",
    "        state = instance.State_Intialization()\n",
    "        sequence = []\n",
    "        # For each sequence, it starts with an initial state, and then iteratively selects actions using the policy \n",
    "        # until a terminal state is reached or a maximum number of steps is taken. \n",
    "        for t in range(max_steps):\n",
    "            if instance.Is_Terminal(state):\n",
    "                break\n",
    "            action = policy[state]\n",
    "            reward_S = instance.Reward_S(state, action)\n",
    "            next_Rounds, probabilities, rewards = [], [], []\n",
    "            for rs in reward_S:\n",
    "                next_Rounds.append(rs[0])\n",
    "                probabilities.append(rs[1])\n",
    "                rewards.append(rs[2])\n",
    "\n",
    "            next_state = random.choices(next_Rounds, probabilities)[0]\n",
    "            reward = rewards[next_Rounds.index(next_state)]\n",
    "            sequence.append((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "# Example usage\n",
    "instance = MDP()\n",
    "policy = {s: instance.Possible_actions(s)[0] for s in instance.Rounds() if not instance.Is_Terminal(s)}\n",
    "sequences = generateSARS(instance, policy, num_sequences=10, max_steps=20)\n",
    "print('Sequences :')\n",
    "print()\n",
    "i=0\n",
    "for sequence in sequences:\n",
    "    print('Sequence', i+1,end=' :')\n",
    "    print()\n",
    "    print(sequence)\n",
    "    print()\n",
    "    i+=1\n",
    "\n",
    "# At each step, it selects the next state and reward using the reward function of the instance and the probabilities associated with each possible next state. \n",
    "# The resulting sequence of SARS tuples is then added to the list of sequences, which is returned at the end of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87bbaaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Monte_Carlo(object):\n",
    "\n",
    "    def __init__(self, No_of_Questions, Rewards):\n",
    "        No_of_Questions = No_of_Questions\n",
    "        Rewards = Rewards\n",
    "\n",
    "    def game(self, policy):\n",
    "        # initialize variables\n",
    "        state = self.State_Intialization()\n",
    "        done = False\n",
    "        rewards = []\n",
    "        # play until the game is over\n",
    "        while not done:\n",
    "            # Selecting tje particular action\n",
    "            action = policy(state)\n",
    "            # Observation of next state and reward after the corresponding action\n",
    "            next_state, reward, done = self.Reward_S(state, action)[0]\n",
    "            # Saving the values\n",
    "            rewards.append(reward)\n",
    "            # Updation of state\n",
    "            state = next_state\n",
    "        # Cummulative reward compuation\n",
    "        cumulative_rewards = [sum(rewards[i:]) for i in range(len(rewards))]\n",
    "        # create a list of (state, action, cumulative reward) tuples\n",
    "        episodes = [(self.State_Intialization(), None, 0)]\n",
    "        for i in range(len(rewards)):\n",
    "            episodes.append((i+1, policy(i+1), cumulative_rewards[i]))\n",
    "        return episodes\n",
    "\n",
    "    def State_Intialization(self):\n",
    "        return 1\n",
    "\n",
    "    def Is_Terminal(self, state):\n",
    "        return True if state == No_of_Questions+1 else False\n",
    "\n",
    "    def Possible_actions(self,state):\n",
    "        return [] if self.Is_Terminal(state) else ['STAY', 'QUIT']\n",
    "\n",
    "    def Reward_S(self, state, action):\n",
    "        return [(state, 1., 0.)] if state > No_of_Questions else [(state+1, Rewards[state][0], Rewards[state][1]), (No_of_Questions+1, 1.-Rewards[state][0], 0.)] if action == 'STAY' else [(No_of_Questions+1, 1.0, Rewards[state][1])] if state <= No_of_Questions else [(state, 1., 0.)]\n",
    "\n",
    "\n",
    "    def Rounds(self):\n",
    "        return range(1, No_of_Questions+2)\n",
    "\n",
    "\n",
    "# Random policy generator function\n",
    "def random_policy(state):\n",
    "    Possible_actions = ['STAY', 'QUIT']\n",
    "    return random.choice(Possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473afd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random game being played : \n",
      "[(1, None, 0), (1, 'STAY', 0.99)]\n",
      "\n",
      "Printing the transposition matrix : \n",
      "0.99   0.01   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "0.0   0.9   0.1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "0.0   0.0   0.8   0.2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "0.0   0.0   0.0   0.7   0.3   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "0.0   0.0   0.0   0.0   0.6   0.4   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "0.0   0.0   0.0   0.0   0.0   0.5   0.5   0.0   0.0   0.0   0.0   \n",
      "\n",
      "0.0   0.0   0.0   0.0   0.0   0.0   0.4   0.6   0.0   0.0   0.0   \n",
      "\n",
      "0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.3   0.7   0.0   0.0   \n",
      "\n",
      "0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.2   0.8   0.0   \n",
      "\n",
      "0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.1   0.9   \n",
      "\n",
      "0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   \n",
      "\n",
      "Vectorical representation of Reward: \n",
      "[    100     500    1000    5000   10000   50000  100000  500000 1000000\n",
      " 5000000       0]\n",
      "\n",
      " Value function : \n",
      "\n",
      "[  63648.10943705  759738.21429313 1598336.23017438 2480745.24693792\n",
      " 3381021.26432234 4292416.05996743 5135175.18440464 5900948.36670179\n",
      " 6043956.04395604 5494505.49450549       0.        ]\n"
     ]
    }
   ],
   "source": [
    "values_list = [v for k, v in Rewards.items()]\n",
    "\n",
    "# Creating an environment\n",
    "Environment = Monte_Carlo(10, values_list)\n",
    "\n",
    "# Creating a random instance or game \n",
    "Instance = Environment.game(random_policy)\n",
    "print('Random game being played : ')\n",
    "print(Instance)\n",
    "print()\n",
    "\n",
    "# # Create the transition matrix\n",
    "no_of_Rounds = No_of_Questions + 1\n",
    "possible_no_of_actions = 2\n",
    "\n",
    "Vector_Reward = np.zeros(no_of_Rounds).astype(int)\n",
    "for num in range(no_of_Rounds):\n",
    "    if num == no_of_Rounds - 1:\n",
    "        # Termination of the game\n",
    "        # so,no reward will be earned\n",
    "        Vector_Reward[num] = 0 \n",
    "    else:\n",
    "        Vector_Reward[num] = values_list[num][1]\n",
    "\n",
    "\n",
    "print(\"Printing the transposition matrix : \")\n",
    "for i in range(no_of_Rounds):\n",
    "    for j in range(no_of_Rounds):\n",
    "        print(Trans[i][j], end = \"   \")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(\"Vectorical representation of Reward: \")\n",
    "print(Vector_Reward)\n",
    "\n",
    "\n",
    "\n",
    "# Let's use Linear Algebra to solve the linear system of equations\n",
    "identity = np.full((no_of_Rounds, no_of_Rounds), 0)\n",
    "np.fill_diagonal(identity, 1)\n",
    "V = np.subtract(identity, Gamma * Trans)\n",
    "b = Vector_Reward\n",
    "# value function\n",
    "V_dash = np.linalg.inv(V)@b \n",
    "\n",
    "# Print the value function vector\n",
    "print(\"\\n Value function : \\n\")\n",
    "print(V_dash)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
